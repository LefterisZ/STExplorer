---
title: "Geographically-inspired Spatial Transcriptomics data analysis using STExplorer"
author: 
  - name: "Eleftherios Zormpas"
    affiliation:
    - "Biosciences Institute, Faculty of Medical Sciences, Newcastle University, UK"
    email: e.zormpas2@ncl.ac.uk, zormpaslef@outlook.com
  - name: "Simon J Cockell"
    affiliation:
      - "School of Biomedical, Nutritional and Sport Sciences, Newcastle University, UK"
    email: simon.cockell@newcastle.ac.uk
# package: STExplorerDev
output: 
  BiocStyle::html_document:
    toc_float: true
bibliography: [references.bib]
vignette: >
  %\VignetteIndexEntry{STExplorer}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>", eval = FALSE
)
```

# Introduction

Welcome to the vignette of STExplorer. STExplorer is a package the performs geographically inspired analysis of spatial transcriptomics data.

## Install package

```{r }
## To install the stable version of the package from Bioconductor run the below:
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("STExplorer")


## To install the development version of the package form GitHub run the below:
if (!require("devtools", quietly = TRUE))
    install.packages("devtools")

devtools::install_github("LefterisZ/STExplorer",
                         auth_token = "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")


## To load the package use:
library(STExplorer)
```

```{r setup, echo=FALSE, eval=FALSE, message=FALSE}
# library(SpatialFeatureExperiment)
# library(tidyverse)
# library(scran)
# library(scater)
# library(ggspavis)
# library(sf)
# library(spdep)
# library(GWmodel)
# library(tidyterra)
# library(ggplot2)
# library(igraph)
# library(pheatmap)
# library(ggExtra)
# library(future)
# library(doFuture)
# library(foreach)
# library(progressr)
# library(parallel)
# library(cols4all)
# library(pheatmap)
# library(RColorBrewer)
```

The STExplorer package utilises methods from Geography's spatial data analysis techniques and packages to perform a spatially-aware and spatially-weighted analysis of spatial transcriptomics data. Below is a series of packages from the field of Geography that STExplorer depends on in one way or another.

-   [`spdep`](https://cran.r-project.org/web/packages/spdep/index.html) is a collection of functions to create spatial weights matrix objects from polygon *contiguities*, from point patterns by distance and tessellations. It is used for summarizing these objects, and for permitting their use in spatial data analysis like regional aggregation and tests for spatial *autocorrelation*.

-   [`sf`](https://cran.r-project.org/web/packages/sf/index.html) (*Simple Features for R*) is a package that offers support for simple features, a standardized way to encode spatial vector data.

-   [`GWmodel`](https://cran.r-project.org/web/packages/GWmodel/index.html) is a suite of models that fit situations when data are not described well by some global model, but where there are spatial regions where a suitably localised calibration provides a better description.

As with every other data analysis approach, data filtering and normalisation is a key step. For this reason STExplorer depends also in packages like [`scater`](https://bioconductor.org/packages/release/bioc/html/scater.html) and [`scran`](Spot-level%20quality%20control%20(sQC)%20procedures%20are%20employed%20to%20eliminate%20low-quality%20spots%20before%20conducting%20further%20analyses) to perform these steps. The choice of the packages was not random. The main reasoning is that we want STExplorer to operate within the Bioconductor environment. The `scater` and `scran` packages are within the Bioconductor environment and are two of the commonly used packages for spatial transcriptomics analysis which include a variety of options to normalise and pre-process data.

-   [`scater`](https://bioconductor.org/packages/release/bioc/html/scater.html) is also a Bioconductor package that is a selection of tools for doing various analyses of scRNA-seq gene expression data, with a focus on quality control and visualization which has extended applications to STx data too. It is based on the `SingleCellExperiment` and `SpatialExperiment` classes and thus is interoperable with many other Bioconductor packages such as [`scran`](Spot-level%20quality%20control%20(sQC)%20procedures%20are%20employed%20to%20eliminate%20low-quality%20spots%20before%20conducting%20further%20analyses), [`scuttle`](https://bioconductor.org/packages/release/scuttle) and [`iSEE`](https://bioconductor.org/packages/release/iSEE).

# STExplorer Analysis pipeline

So far we had an introduction to the data structures used in the `STExplorer` analysis pipeline. In the next chapters we will guide you step by step through the analysis of spatial transcriptomics data using the `STExplorer` package.

## Loading a dataset

For this vignette we will be using the human steatotic liver dataset from the [Liver Atlas](https://livercellatlas.org/index.php) [@GUILLIAMS2022379]. Specifically we use the JBO019 and JBO022 samples (parts of these samples) to showcase the STExplorer use. The JBO019 sample is steatotic (diseased) while the JBO022 is healthy.

First we generate the `MetaSpatialFeaturesExperiment` (msfe) object which includes two `SpatialFeaturesExperiment` (sfe) objects that include 1 sample each.

**NOTE 1:** At the moment, the SFE class is not behaving right when we include multiple samples in it. It has some problems with saving the images inside RDS/RDA files, subsetting the sfe object and adding annotations inside colData(). It looks like a `SpatialFeatureExperiment` package problem that we don't have the time to investigate further at the moment. As a result, we kindly ask you to use *ONE sfe object for ONE sample*.

**NOTE 2:** The `read10xVisiumSFE` function expects a specific folder structure. Please make sure your folders look like this:

-   data
    -   Healthy
        -   Sample1
            -   outs
                -   filtered_feature_bc_matrix.h5
                -   spatial
                    -   scalefactors_json.json
                    -   tissue_hires_image.png
                    -   tissue_lowres_image.png
                    -   tissue_positions_list.csv

The `outs` folder and its contents **must** have these names. What will change is the `.h5` file. This can be `raw_feature_bc_matrix.h5` if the count data include off-tissue spots as well, or it can be a folder named `raw`/`filtered_feature_bc_matrix`. In that case, the count data have been extracted and are in a form of a sparse matrix. This means that when we import the data below, we will provide the character string `"sparse"` to the `type` argument of the `read10xVisiumSFE` function. Additionally, the `feature_bc_matrix` folder it is expected to have the below structure:\

-   filtered_feature_bc_matrix
    -   barcodes.tsv.gz
    -   features.tsv.gz
    -   matrix.mtx.gz

```{r 03_load_sfe, warning=FALSE, message=FALSE}
## Create the MSFE object
msfe <- MetaSpatialFeatureExperiment()

sampleDir <- c("./data/test_data/Visium_Human_Liver/Healthy/JBO022_Results",
               "./data/test_data/Visium_Human_Liver/Steatotic/JBO019_Results")

sampleNames <- c("JBO022", "JBO019")

## Load sfe objects inside the msfe
## The `addSFE` function is appending each sfe to a new slot inside the msfe.
## Do it one by one. I tried a for loop but for a reason it overwrites the msfe instead of appending. 
msfe <- addSFE(msfe,
               read10xVisiumSFE(samples = sampleDir[1], 
                      sample_id = sampleNames[1], 
                      type = "HDF5", 
                      data = "filtered", 
                      images = "lowres", 
                      style = "W", 
                      zero.policy = TRUE))

## BEWARE: There is a chance that you get the below error if you run this chunk within RMarkdown:
## Error in names(xyz) <- names(sfs) <- sids : 
##  'names' attribute [1] must be the same length as the vector [0]
##
## OR:
## "error in evaluating the argument 'x' in selecting a method for function 'colData': 'names' attribute [1] must be the same length as the vector [0]"
##
## If you see this, then it is because of two reasons:
##    1. You are missing a folder named 'outs' at the end of the pathway you provided earlier. --> add outs folder
##    2. If path is correct, then try running it in the console -I don't know why it behaves like 
##       this and it is not even a function from STExplorer. It is from the
##       'SpatialFeatureExperiment' package.

## NOTE1: when formatting the ground truth table to import it make sure that:
##      (a) the table includes the column names "Barcode", "sample_id", and "annotation"
##      (b) the Barcodes are stripped of their suffix if there are multiple samples. 
##          Probably they will look like this: "ATCGGCTAGCTGAT-1_1". 
##          Remove the "_1" from the end.
## NOTE2: make sure at the end to have a separate data frame with annotations per sample.
##        Below I am loading them into a list.
ground_truth <- read_table("./data/test_data/Visium_Human_Liver/spotZonationGroup.txt")

gTruth_list <- list(JBO022 = ground_truth[ground_truth$sample_id == "JBO022",],
                    JBO019 = ground_truth[ground_truth$sample_id == "JBO019",])

str(gTruth_list)
## If the data frames in the list are empty, please check you have no typos in the sample ids.
```

## Spot-level Quality Control

Considered quality control (QC) procedures are essential for analysing any high-throughput data in molecular biology. The removal of noise and low quality data from complex datasets can improve the reliability of downsrtream analyses. STx is no different in this regard, and QC can be undertaken in 2 main places - spot-level and gene-level. Here, we focus on spot-level QC.

Spot-level quality control (sQC) procedures are employed to eliminate low-quality spots before conducting further analyses. Low-quality spots may result from issues during library preparation or other experimental procedures, such as a high percentage of dead cells due to cell damage during library preparation, or low mRNA capture efficiency caused by ineffective reverse transcription or PCR amplification. Keeping these spots usually leads to creating problems during downstream analyses.

We can identify low-quality spots using several characteristics that are also used in cell-level QC for scRNA-sq data, including:

1.  **library size** (total of UMI counts per spot will vary due to sequencing *-like different samples in a bulk RNA-seq-*, or due to number of cells in the spot)
2.  **number of expressed genes** (i.e. number of genes with non-zero UMI counts per spot)
3.  **proportion of reads mapping to mitochondrial genes** (a high proportion indicates putative cell damage)

Low library size or low number of expressed features can indicate poor mRNA capture rates, e.g. due to cell damage and missing mRNAs, or low reaction efficiency. A high proportion of mitochondrial reads indicates cell damage, e.g. partial cell lysis leading to leakage and missing cytoplasmic mRNAs, with the resulting reads therefore concentrated on the remaining mitochondrial mRNAs that are relatively protected inside the mitochondrial membrane. Unusually high numbers of cells per spot can indicate problems during cell segmentation.

The idea of using scRNA-seq QC metrics in STx data comes from the fact that if we remove space and effectively treat each spot as a single cell, the two datasets share common features. We need to bear in mind, however, that the expected distributions for high-quality *spots* are different (compared to high-quality *cells* in scRNA-seq), since spots may contain zero, one, or multiple cells.

A few publications for further reading that can help you understand the quality controls: @McCarthy2017Apr and @Amezquita2020Feb.

### Calculating QC metrics

We will calculate the three main QC metrics described above using methods from the `scater` [@McCarthy2017Apr] package, and investigate their influence on the DLPFC dataset with some plots from `ggspavis`, along with some additional plots of our own.

At present, the dataset contains both on- and off-tissue spots - we plotted these in the previous practical. For any future analysis though we are only interested in the on-tissue spots. Therefore, before we run any calculations we want to remove the off-tissue spots.

***NOTE***: the on- or off-tissue information for each spot can be found in the `colData` of the `sfe` object and in the `in_tissue` column where *0 = off-tissue* and *1 = on-tissue*.

```{r 02_keep_on-tissue}
## Dataset dimensions before the filtering
dim(msfe@sfe_data)

```

The next thing we need to do before we make decisions on how to quality *"trim"* the dataset is to calculate the percentage per spot of mitochodrial gene expression and store this information inside the `colData`. First of all, find the mitochrondrial genes - their gene names start with "MT-" or "mt-".

Then find what proportion of reads in a spot's library are attributable to the expression of these genes. This uses a function, `addPerCellQC()` from `scater` (which in this instance is actually a wrapper around `scuttle`).

```{r 03_QC_sfe1, message=FALSE, warning=FALSE}
is_mito <- grepl("(^MT-)|(^mt-)", rowData(sfe)$gene_name)
sfe <- addPerLocQC(sfe, gTruth = ground_truth, assay = "counts", 2, subsets = list(mito = is_mito))
sfe <- addGeometries(sfe, samples = sampleDir, sample_id = sampleNames, res = "fullres")
sfe <- addPerGeneQC(sfe, assay = "counts", version = NULL, mirror = NULL)

## Keep in-tissue locations
sfe <- filterInTissue(sfe, sample_id = TRUE)

colData(sfe)
rowData(sfe)
colGeometries(sfe)
```

After calculating a required metric, we need to apply a cut-off threshold for the metric to decide whether or not to keep each spot. It is important to consider an individual dataset on its own merits, as it might need slightly different cut-off values to be applied. As a result we cannot rely on identifying a single value to use every time and we need to rely on plotting these metrics and making a decision on a dataset-by-dataset basis.

### Plot tissue map {#plot-tissue-map}

The dorso-lateral prefrontal cortex (DLPFC) is a functional brain region in primates involved in executive function. It consists of six layers of neurons that differ in their cell types, density and connections. The DLPFC dataset we looked at in session one, and will be here using comes with manual annotation of these layers (and the adjacent white matter - WM) by the authors @Maynard2021Mar. We can plot the tissue map with and without the annotations to get a complete view.

```{r 02_plot-maps-gTruth, fig.show = 'hold', out.width="50%", fig.height=5, fig.width=4}
## Plot spatial coordinates without annotations
plotQC_spots(sfe, type = "spot", sample_id = TRUE, in_tissue = FALSE, colours = NULL)
plotQC_spots(sfe, type = "spot", sample_id = NULL, in_tissue = TRUE)
plotQC_spots(sfe, type = "spot", sample_id = TRUE, in_tissue = FALSE, colours = c("#3C5338", "#FF9999"))
plotQC_spots(sfe, type = "spot", sample_id = "JBO022", in_tissue = TRUE)

## Plot spatial coordinates with annotations
plotQC_spotsAnnotation(sfe = sfe, type = "spot", sample_id = TRUE)
plotQC_spotsAnnotation(sfe = sfe, type = "spot", sample_id = NULL)
plotQC_spotsAnnotation(sfe = sfe, type = "spot", sample_id = "JBO019")
```

### Plot manual annotation with tissue image

```{r 03_QC_sfe2, message=FALSE, warning=FALSE}
plotQC_tissueImg(sfe, res = "lowres", type = "spot", sample_id = TRUE, annotate = TRUE, alpha = 0.3)
plotQC_tissueImg(sfe, res = "lowres", type = "spot", sample_id = TRUE, annotate = FALSE, alpha = 0.3)
plotQC_tissueImg(sfe, res = "lowres", type = "hex", sample_id = NULL, annotate = TRUE, alpha = 0.3)
plotQC_tissueImg(sfe, res = "lowres", type = "hex", sample_id = NULL, annotate = FALSE, alpha = 0.3)
plotQC_tissueImg(sfe, res = "lowres", type = "none", sample_id = "PrsCncA1", annotate = FALSE)

```

### Library size threshold

We can plot a histogram of the library sizes across spots. The library size is the number of UMI counts in each spot. We can find this information in the `sum` column in the `colData`.

As we can see there are no obvious issues with the library sizes. An example of an issue could be a high frequency of small libraries which would indicate poor experimental output. Generally we do not want to keep spots with too small libraries.

If the dataset we are analysing contains the number of cells that are present in each spot (this one does), then it makes sense to also plot the library sizes against the number of cells per spot. In that way we are making sure that we don't remove any spots that may have biological meaning. In many cases though the datasets do not have such information unless we can generate it using a nuclei segmentation tool to extract this information from the H&E images.

The horizontal red line (argument `threshold` in the `plotQC` function) shows a first guess at a possible filtering threshold for library size based on the above histogram.

```{r 03_QC_sfe3}
# ----------------------------------------------- #
## Density and histogram of library sizes
plotQC_hist(sfe, metric = "libsize")
plotQC_hist(sfe, metric = "libsize", limits = c(3000, 36500))
plotQC_hist(sfe, metric = "libsize", limits = c(3000, 36500),
            hist_args = list(bins = 100),
            dens_args = list(alpha = 0.5,
                             adjust = 0.5,
                             fill = "#F0AAA8"),
            vline_args = list(colour = "blue",
                             linetype = "dashed"))

## Scatter plot library sizes vs number of cells
plotQC_scat(sfe, metric = "libsize")

## Select threshold
sfe <- setQCthresh_LibSize(sfe, sample_id = TRUE, min_t = 2000, max_t = 35000)

## Check putative spatial patterns of removed spots
plotQC_filtered(sfe, metric = "libsize", sample_id = TRUE)
```

**NOTE**: using `ggExtra` we provide the marginal histograms here.

We need to keep in mind here that the threshold is, to an extent, arbitrary. It is therefore important to look at the number of spots that are left out of the dataset by this choice of cut-off value, and also have a look at their putative spatial patterns. If we filtered out spots with biological relevance, then we should observe some patterns on the tissue map that correlate with some of the known biological structures of the tissue. If we do observe such a phenomenon, we have probably set our threshold too high (i.e. not permissive enough).

### Number of expressed genes

As we did with the library sizes, we can plot a histogram of the number of expressed genes across spots. A gene is "expressed" in a spot if it has at least one count in it. We can find this information in the `detected` column in the `colData`.

We will follow the same logic for the plots as we did for the library size earlier.

Finally, again as before, we apply the chosen threshold to flag spots with (in this case) fewer than 500 expressed genes.

```{r 03_QC_sfe4}
# ----------------------------------------------- #
## Density and histogram of expressed genes
plotQC_hist(sfe, metric = "detected")
plotQC_hist(sfe, metric = "detected", limits = c(1000, NA))

## Scatter plot expressed genes vs number of cells
plotQC_scat(sfe, metric = "detected")

## Select threshold
sfe <- setQCthresh_GenesExpr(sfe, sample_id = TRUE, min_t = 700, max_t = NA)

## Check putative spatial patterns of removed spots
plotQC_filtered(sfe, metric = "detected", sample_id = TRUE)
```

**NOTE:** For reference, remember the ground truth layers in this dataset [that we plotted](#plot-tissue-map) at the beginning of this session.

### Percentage of mitochondrial expression

As we briefly touched on at the beginning, a high proportion of mitochondrial reads indicates low cell quality, probably due to cell damage.

We calculated this data earlier on in this session, and can now investigate the percentage of mitochondrial expression across spots by looking at the column `subsets_mito_percent` in the `colData`.

In this instance, a higher percentage of mitochondrial expression is the thing to avoid, so the threshold is an upper bound, rather than the lower bounds we have observed so far. Our suggestion this time is to cut-off at 28%.

```{r 03_QC_sfe5}
# ----------------------------------------------- #
## Density and histogram of percentage of mitochondrial expression
plotQC_hist(sfe, metric = "mito")
plotQC_hist(sfe, metric = "mito", limits = c(NA, 20))

## Scatter plot % mito expression vs number of cells
plotQC_scat(sfe, metric = "mito")

## Select threshold
sfe <- setQCthresh_Mito(sfe, sample_id = TRUE, min_t = NA, max_t = 21)

## Check putative spatial patterns of removed spots
plotQC_filtered(sfe, metric = "mito", sample_id = TRUE)
```

### Number of cells per spot

As previously mentioned, number of cells per spot is an attribute that not all datasets include. Nonetheless, it can be useful to further control the quality of the dataset prior to any downstream analysis. Of course, the number of cells per spot depends on the tissue type and organism and according to [10X Genomics](https://kb.10xgenomics.com/hc/en-us/articles/360035487952-How-many-cells-are-captured-in-a-single-spot-), each spot typically contains between 0 and 10 cells.

The DPFLC dataset does contain information on the number of cells per spot (acquired by processing and cell segmentation of high-resolution histology images obtained prior on-slide cDNA synthesis, see @Maynard2021Mar for details). To investigate the number of cells in each spot looking for any outlier values that could indicate problems we need to take a look in the column `cell_count` in `colData`.

```{r 02_plot-cellsPerSpot-histo, fig.height=4, warning=FALSE, message=FALSE}
## Density and histogram of the number of cells in each spot
plotQC_hist(sfe, metric = "cellCount")

```

```{r 02_cellsPerSpot-scatter, fig.width=6, fig.height=5, warning=FALSE, message=FALSE}
## plot number of expressed genes vs. number of cells per spot
plotQC_scat(sfe, metric = "detected")
```

As we can see from both the histogram and the scatter plot there is a tail of very high values, which could indicate problems for these spots. More specifically, we can see from the scatter plot that most of the spots with very high cell counts also tend to have lower numbers of expressed genes. This indicates problems with the experiment on these spots, and they should be removed.

```{r 02_cellsPerSpot-thresh, fig.height=4}
## Select threshold
sfe <- setQCthresh_CellCount(sfe, sample_id = TRUE, min_t = NA, max_t = 25)
## Check putative spatial patterns of removed spots
plotQC_filtered(sfe, metric = "cellCount", sample_id = TRUE)

```

While there is a spatial pattern to the discarded spots, it does not appear to be correlated with the known biological features (cortical layers). The discarded spots are typically at the edges of the tissue. It seems plausible that something has gone wrong with the cell segmentation on the edges of the images, so it makes sense to remove these spots.

### Remove low-quality spots

All the steps so far have flagged spots with potential issues - before proceeding with analysis, we want to remove these spots from our SpatialExperiment object. Since we have calculated different spot-level QC metrics and selected thresholds for each one, we can combine them to identify a set of low-quality spots, and remove them from our `sfe` object in a single step.

If the dataset has also manual annotation ([remember](#plot-tissue-map))) we see that there are locations that are not annotated (marked with `NA`). We could further remove those locations to reduce potential noise and further increase the quality of the dataset.

```{r 02_notAnnotSpots, fig.height=4}
## Select locations without annotation
sfe <- setQCthresh_NAs(sfe, sample_id = TRUE)
## Check putative spatial patterns of removed spots
plotQC_filtered(sfe, metric = "NAs", sample_id = TRUE)
```

We can also check once more that the combined set of discarded spots does not correspond to any obvious biologically relevant group of spots.

```{r 03_QC_sfe6}
# ----------------------------------------------- #
## Set the combined filtering threshold using the QC metrics
sfe <- setQCtoDiscard_loc(sfe, sample_id = TRUE, filters = TRUE)

## Check putative spatial patterns of removed spots
plotQC_filtered(sfe, metric = "discard", sample_id = TRUE)

# ----------------------------------------------- #
## Remove combined set of low-quality spots
sfe <- applyQCthresh_loc(sfe, sample_id = TRUE)
```

## Normalisation of counts

### Background

Normalisation is applied in STx data for the same reason as any other RNA-Seq technique - the differences observed in the count data can arise from a range of systematic factors, not just a physiologically-relevant change in expression. The primary systematic effect is that of library size (or in the case of STx, counts/UMIs per spot). `scater` corrects for library size by scaling the sizes across all spots such that the mean library size is 1. Normalized counts are then calculated as a ratio of observed count to library size factor.

Secondly, a log-transformation is applied to the scaled counts - this transformation is commonly applied as it stabilises the variance across the range of transcriptomics data (otherwise the variance is dominated by highly expressed genes) and it facilitates comparisons of expression by rendering positive and negative changes symmetrical and found by subtraction rather than division. Since $log2(0)$ is undefined, a *pseudocount* is added to each observed count to avoid this error - a pseudocount of 1 is typically applied, as $log2(0+1) = 0$.

Here we will be using methods from the `scater` [@McCarthy2017Apr] and `scran` [@Lun2016Oct] packages that calculate logcounts using library size factors. The library size factors approach is arguably the simplest approach for STx data. Other approaches used in scRNA-seq are more difficult to justify their use in STx because of two main reasons:

1.  Spots can contain multiple cells of different cell-types.
2.  Datasets can include multiple tissue samples which will lead to different clusterings.

### Log-tranformation of counts

```{r 03_LogNorm_sfe}
## Calculate library size factors
sfe <- computeLibSizeFactors(sfe, type = "intra")
```

As described above, the mean size factor is 1.0.

```{r 03_LogNorm_sfe2}
## Density and histogram of library sizes
plotQC_sizeFactors(sfe)
```

The log-transformation that takes place is a log2-transformation and in order to avoid *- Infinity* values we add a pseudo value of 1. Both the log2- transformation and the pseudocount of 1 are defaults in this method.

```{r 03_LogNorm_sfe3}
# Generate a metaSFE object 
meta_sfe <- asMetaSFE(sfe)

## Calculate logcounts using library size factors
meta_sfe <- normaliseCounts(msfe = meta_sfe)

## Check that a new assay has been added
assayNames(meta_sfe[[1]])
```

## Gene-level Quality Control

### Calculating extra QC metrics

```{r 03_GeneQC_sfe1}
## Calculate the mean of log counts over the number of locations a gene is present
meta_sfe <- perGeneLogMean(meta_sfe)
```

### Set and apply filters

**NOTE** - Feature selection is a complicated process with significant impacts on the chosen downstream analysis.

```{r 03_GeneQC_sfe2}
## Zero expression genes
meta_sfe <- setQCthresh_ZeroExpr(meta_sfe)

## Lowly expressed (noise?!) genes
meta_sfe <- setQCthresh_LowLogMean(meta_sfe)

## Remove mitochondrial and other genes
meta_sfe <- setQCthresh_custom(meta_sfe, sample_id = TRUE, 1, qcMetric = is_mito)

## QC discard Features
## Set the combined filtering threshold using the QC metrics
meta_sfe <- setQCtoDiscard_feat(meta_sfe, filters = TRUE)

## FEATURE SELECTION
## Apply gene-level QC threshold
meta_sfe <- applyQCthresh_feat(meta_sfe)
```

In this dataset, the mitochondrial genes are too highly expressed and are not of major biological interest. As a result, if we are to identify true HVGs, we first need to remove the mitochondrial genes.

## Selecting genes

### Background

Gene selection - or alternatively "feature selection" - is applied to identify genes that are likely to be informative for downstream analyses. The most common feature selection method is the definition of highly variable genes (HVGs). The assumption is that since we quality-controlled and normalised our dataset, the genes with high variability are the ones that contain high levels of biological variability too. Since here we have a spatial dataset we can also try to identify spatially variable genes too (SVGs).

It is important to note that HVGs are identified solely from the gene expression data. Spatial information does not play a role in finding HVGs. STx data pose a dilemma; does the meaningful spatial information reflect only spatial distribution of major cell types or does it reflect additional important spatial features? If we believe the former, relying on HVGs can be enough. If the second also holds true though, it is important to identify SVGs as well.

### Highly Variable Genes (HVGs)

Here we will be using methods from the `scran` package [@Lun2016Oct] to identify a set of HVGs. Again, here we need to remember that `scran` methods were developed for scRNA-seq and we are performing the analysis under the assumption that the spots of an STx experiment can be treated as single cells.

Then, we apply methods from `scran` that give a list of HVGs, which can be used for further downstream analyses.

First we model the variance of the log-expression profiles for each gene, decomposing it into technical and biological components based on a fitted mean-variance trend.

```{r 03_HVGs_sfe}
## Fit mean-variance relationship
dec <- modelGeneVariance(meta_sfe, method = "Var")

## Select top HVGs
top_hvgs <- getTopHighVarGenes(dec,
                               var.field = "bio",
                               prop = 0.5,
                               var.threshold = 0,
                               fdr.threshold = 0.1)

## Visualize mean-variance relationship
plotGeneVariance(dec = dec, hvgs = top_hvgs)

```

The `trend` function that we used above is returned from the `modelGeneVar` function and returns the fitted value of the trend at any value of the mean. The "biological" variance of a gene is what remains when the fitted variance for a gene of that expression value is subtracted from the total variance (so genes above the blue trend line have a positive biological variance).

We select the top 10% of genes based on their biological variability The parameter `prop` defines how many HVGs we want. For example `prop = 0.1` returns the top 10% of genes. `prop = 1.0` would return all genes with a positive biological variability.

### Spatially variable genes (SVGs)

SVGs are genes with a highly spatially correlated pattern of expression, which varies along with the spatial distribution of a tissue structure of interest. This phenomenon is also called *spatial autocorrelation* and underlies all types of spatial data, as we will discuss later.

The field of geography has developed some statistical measures to calculate spatial autocorrelation. Examples of these are Moran's *I* [@Moran1950Jun] and Geary's *C* [@Geary1954Nov] that can be used to rank genes by the observed spatial autocorrelation to identify SVGs.

Several sophisticated new statistical methods to identify SVGs in STx data have also recently been developed. These include [SpatialDE](https://github.com/Teichlab/SpatialDE) [@Svensson2018May], [SPARK](https://xzhoulab.github.io/SPARK/) [@Sun2020Feb], and [SPARK-X](https://xzhoulab.github.io/SPARK/) [@Zhu2021Dec].

### Integration of HVGs and SVGs

A recent benchmark paper [@Li2022Jan] showed that integrating HVGs and SVGs to generate a combined set of features can improve downstream clustering performance in STx data. This confirms that SVGs contain additional biologically relevant information that is not captured by HVGs in these datasets. For example, a simple way to combine these features is to concatenate columns of principal components (PCs) calculated on the set of HVGs and the set of SVGs (excluding overlapping HVGs), and then using the combined set of features for further downstream analyses [@Li2022Jan].

## Neighbour graph and distance matrix

### Adding spatial weights

The neighbour lists can be supplemented with spatial weights using the `nb2listw` and `nb2listwdist` function from `spdep` package for the chosen type and coding scheme style. There are 6 different coding scheme styles that can be used to weigh neighbour relationships:

1.  **B**: is the basic binary coding (1 for neighbour, 0 for no neighbour).
2.  **W**: is row standardised (sums over all links to n).
3.  **C**: is globally standardised (sums over all links to n).
4.  **U**: is equal to C divided by the number of neighbours (sums over all links to unity).
5.  **S**: is the variance-stabilizing coding scheme (sums over all links to n).
6.  **minmax**: divides the weights by the minimum of the maximum row sums and maximum column sums of the input weights; It is similar to the C and U styles.

The coding scheme style is practically the value each neighbour will get. For example, in a binary coding scheme style (**B**) if a spot is a neighbour of the spot in focus then gets the value of **1**, else gets **0**. Another example, in a row standardised coding scheme style (**W**) if the spot in focus has a total of 10 neighbours and each neighbour has a weight of 1, then the sum of all neighbour weights is 10, and each neighbour will get a normalised weight of 1/10 = 0.1. As a result, in the row standardised coding scheme, spots with many neighbours will have neighbours with lower weights and thus will not be over-emphasised.

Starting from a binary neighbours list, in which regions are either listed as neighbours or are absent (thus not in the set of neighbours for some definition), we can add a distance-based weights list. The `nb2listwdist` function supplements a neighbours list with spatial weights for the chosen types of distance modelling and coding scheme. While the offered coding schemes parallel those of the `nb2listw` function above, three distance-based types of weights are available: inverse distance weighting (IDW), double-power distance weights (DPD), and exponential distance decay (EXP). The three types of distance weight calculations are based on pairwise distances 𝑑𝑖𝑗, all of which are controlled by parameter *"alpha"* (𝛼 below):

1.  **idw**: 𝑤𝑖𝑗=𝑑−𝛼𝑖𝑗,
2.  **exp**: 𝑤𝑖𝑗=exp(−𝛼⋅𝑑𝑖𝑗),
3.  **dpd**: 𝑤𝑖𝑗=[1−(𝑑𝑖𝑗/𝑑max)𝛼]𝛼,

the latter of which leads to 𝑤𝑖𝑗=0 for all 𝑑𝑖𝑗\>𝑑max. Note that *IDW* weights show extreme behaviour close to 0 and can take on the value infinity. In such cases, the infinite values are replaced by the largest finite weight present in the weights list.

25### Generate distance matrices

A distance matrix is a mirrored matrix that contains the distance between a spot and every other spot. This distance can be a simple Euclidean distance based on the coordinates of the spots or a weighted distance according to a bandwidth around each spot using a kernel that gives higher scores to distances between spots that are closer together compared to the ones that are farther away. These weighted distance matrices are later used to run geographically weighted (GW) models.

There are 6 different kernels that can be used to weight the distances between spots. The next two figures are from the `GWmodel` publication [@Gollini2015Feb] and illustrate the mathematical application of these kernals, and show graphically how they weight by distance.

```{r GWmodelFig1, echo=FALSE, out.width = "100%", fig.align="center", fig.cap="The math equations that define the kernels."}
knitr::include_graphics("images/gwmodel_kernel_math.png")
```

```{r GWmodelFig2, echo=FALSE, out.width = "100%", fig.align="center", fig.cap="Examples from using each kernel."}
knitr::include_graphics("images/gwmodel_kernel_graphs.png")
```

In the below we choose one of the many possible ways of building a neighbour graph for the steatotic liver data set. In this example we are using a k-nearest neighbours approach with row-standardised distance-based weights.

```{r 03_spatial_weights_to_sfe}
## Add a neighbour graph using a weighted distance matrix
meta_sfe <- addSpatialNeighGraphs(meta_sfe, sample_id = TRUE, type = "knearneigh", style = "W", distMod = "raw", k = 6)

colGraphs(meta_sfe[[1]])

## Calculate a simple distance matrix
meta_sfe <- addDistMat(meta_sfe, p = 2)

```

We can use a `geom` from the `tidyterra` package (commonly used for map visualisations) to plot the neighbour graph we generated in the previous step.

```{r 03_visualise_neighbours}
## Plot the neighbours graph
plotNeighbourGraph(meta_sfe, sample_id = TRUE,
                   res = "lowres", plotImage = TRUE)
plotNeighbourGraph(meta_sfe, sample_id = "JBO019",
                   res = "lowres", plotImage = TRUE)
plotNeighbourGraph(meta_sfe, sample_id = "JBO019",
                   res = "lowres", plotImage = FALSE)
```

Now that we have a fully QC-ed dataset with spatial weights and a neighbour graph applied, we have prepared our data fully for the application of geospatial methods - specifically in practical 4, geogrpahically weighted principal components analysis (GWPCA).

## Geographically Weighted Principal Components Analysis (GWPCA)

A standard PCA can pick out the key multivariate modes of variability in the data. Looking at outlying values of the principal components of these data gives us an idea of unusual sites (in terms of combinations of gene expression profiles - and to a certain extent of combinations of cell types in each spot). Next, geographically weighted PCA can be used to find spatial multivariate outliers. Sounds complicated, but really all this means is it identifies sites that have an unusual multi-way combination of gene expression in relation to their immediate geographical neighbours. It might be that the values observed at these sites as a combination is not uncommon in the tissue as a whole - but is very unusual in its locality.

To find such outliers the procedure is relatively simple - instead of doing a PCA on the tissue as a whole, for each sample we do a PCA on data falling into a window centred on the location of that spot. In that way we can check whether the spot is like its neighbours or not, from a multivariate viewpoint.

The procedure we will follow in this practical carries out a geographically weighted PCA. In short, it runs a 'windowed' PCA around each of the spots.

### Parameter prearation for GWPCA

The `gwpca` method uses `princomp` internally to run the PCAs - this function does not allow the number of variables (genes) to be greater than the number of samples (spots). This imposes a hard requirement on the data pre-processing. We have, however, already identified the highly variable genes in our sample, and for this case, there are fewer genes than spots.

Some other parameterisation is neccessary and these required parameters (as we have used for this dataset) are illustrated here:

```{r 04_set_parameters, eval=FALSE}
## Select the sample you would like to perform a GWPCA analysis
sfe <- getSFE(meta_sfe, "JBO019")
## Get the gene names that are going to be evaluated
vars = top_hvgs
## Set a fixed bandwidth
bw = 6*sfe@metadata[["spotDiameter"]][["JBO019"]][["spot_diameter_fullres"]]
## Set the number of components to be retained
k = 20
## Set the kernel to be used
kernel = "gaussian"
## Set the Minkowski distance power: p = 2 --> Euclidean
p = 2
## Is the bandwidth adaptive?: No because spots are fixed
adaptive = FALSE
## Cross-Validate GWPCA?
cv = TRUE
## Calculate PCA scores?
scores = FALSE
## Run a robust GWPCA?
robust = FALSE
## Make a cluster for parallel computing (otherwise GWPCA is slow!)
my.cl <- makeClusterGWPCA(type = "FORK")
```

The bandwidth defines a radius around each spot - every spot that falls inside this radius is considered a neighbour. We can set bandwidth as a fixed value (as here) or we can select the bandwidth automatically. Without going into detail here, this is achieved by a form of cross validation, where each observation is omitted, and it is attempted to reconstruct the values on the basis of principal components, derived from the other observations. The bandwidth achieving the optimal results is the one selected. For a complete explanation, see @Harris2011Oct. The function `bw.gwpca` from `GWmodel` can be used to computes this.

-   **NOTE**: Larger bandwidths imply bigger moving spatial windows, which in turn imply smoother spatially varying outputs.

### Run GWPCA

Here we present the invocation to run GWPCA, however because this process is computationally intensive and time-consuming, we do not suggest running it on posit.cloud. We have pre-computed the result and provide it for you to load.

```{r 04_run_gwpca1, eval=FALSE}
# Run GWPCA
pcagw <- gwpcaSTE(sfe = sfe, 
                  assay = "logcounts",
                  vars = vars, 
                  p = p, 
                  k = k, 
                  bw = bw, 
                  kernel = kernel,
                  adaptive = adaptive, 
                  scores = scores, 
                  robust = robust,
                  cv = cv,
                  future = FALSE,
                  strategy = "cluster",
                  workers = my.cl,
                  verbose = FALSE)

```

### Plot global PCA results

In the next steps we will take a look inside the output from the `gwpca` function and we are going to extract some basic information. Since GWPCA consists of multiple local PCAs, it is good to know how many PCs makes sense to look at. We can do so by running a global PCA and plotting a scree plot:

```{r 04_scree_plot, fig.height=3, fig.width=8}
plotGWPCA_global(gwpca = pcagw,
                 comps = 1:10,
                 type = "scree",
                 point_args = list(size = 3, colour = "red"),
                 line_args = list(linewidth = 1, colour = "dodgerblue"))
```

In a Principal Component Analysis (PCA), the first three principal components may explain less than 15% of the variance in the data if the data is highly dispersed or if there is a large amount of noise in the data. This means that the first three principal components are not capturing a significant portion of the variability in the data. This could be due to a lack of clear structure in the data or a lack of meaningful patterns that can be captured by the PCA. Alternatively, it could be due to the presence of many irrelevant features or variables in the data that are not contributing to the overall variance. This is one more of the reasons why GWPCA is more appropriate for STx data. Because, it may be true that the global PCs are not strong but locally this can change.

### Identify the leading genes in each location

The genes with the highest loading scores (where loading score = correlation between variable and component) at each location can be thought of as the "leading genes" - i.e. those with the most explanatory power with respect to the variability of gene expression at that location. These leading genes can be a local indicator of relevant biology.

Here we look at leading genes in 2 ways - (1) by finding the single gene with the highest loading at each location; (2) by finding sets of the top 4 genes by loading score, where the order of those genes does not matter (so the ordered set A,B,C,D is considered the same as D,B,A,C).

```{r leading_genes1}
## Extract leading genes
pcagw <- gwpca_LeadingGene(gwpca = pcagw, 
                           m_sfe = sfe, 
                           pc_nos = 1:4, 
                           type = "single", 
                           names = "gene_names")

pcagw <- gwpca_LeadingGene(gwpca = pcagw, 
                           m_sfe = sfe, 
                           pc_nos = 1:4, 
                           genes_n = 4, 
                           type = "multi", 
                           method = "membership", 
                           names = "gene_names")
```

We can also plot these leading genes on the spot map - as each location by definition has (potentially) a different leading gene.

```{r leading_genes2, fig.show = 'hold', out.width='.49\\linewidth', fig.asp=1, fig.ncol = 1}
## Plot leading genes
plotGWPCA_leadingG(gwpca = pcagw,
                   comps = 1:2,
                   type = "single",
                   arrange = FALSE)

plotGWPCA_leadingG(gwpca = pcagw,
                   comps = 1,
                   type = "multi",
                   arrange = FALSE)
```

The "multi" plot here is problematic, because there are too many groups of genes to be able to print a legible legend. Although the `plotGWPCA_leadingG` function by default highlights gene groups that are present in at least 12 locations, however, provides more arguments to deal with this. The below code snippet is provided to highlight gene groups that are found in at least 12 spots, change the location of the legend, and adjust the legend text size.

```{r leading_genes3, fig.show = 'hold', fig.asp=1, fig.ncol = 1}
### Plot multi type (extra parameters)
plotGWPCA_leadingG(gwpca = pcagw,
                   comps = 1,
                   type = "multi",
                   arrange = FALSE,
                   legend.position = "bottom",
                   cutoff = 12,
                   size = 8)
```

### Percentage of Total Variation (PTV)

Another useful diagnostic for PCA is the percentage of variability in the data explained by each of the components. Locally, this can be achieved by looking at the `local.PV` component of `pcagw`; this is written as `pcagw$local.PV`. This is an 1161 by 20 matrix - where 1161 is the number of observations and 20 is the number of components (`k`). For each location, the 20 columns correspond to the percentage of the total variance explained by each of the principal components at that location. If, say, the first two components contributed 90% of the total variance, then it is reasonable to assume that much of the variability in the data can be seen by just looking at these two components. Because this is geographically weighted PCA, this quantity varies across the map.

```{r 04_ptv, fig.show='hold'}
## Calculate the PTV for multiple Components
pcagw <- gwpca_PropVar(gwpca = pcagw, n_comp = 2:10, m_sfe = sfe)

## Plot PTV
plotGWPCA_ptv(gwpca = pcagw,
              comps = 1:10,
              type = "violin")

## Map PTV
plotGWPCA_ptv(gwpca = pcagw,
              comps = 1:6,
              type = "map")
```

### Identify discrepancies

Global PCA can be used to identify multivariate outliers. Extending this, it is also possible to use local PCA (i.e., GWPCA) to identify local outliers. One way of doing this links back to the cross-validation idea that can be used to select a bandwidth. Recall that this is based on a score of how well each observation can be reconstructed on the basis of local PCs. The score measures the total discrepancies of true data values from the reconstructed ones - and the bandwidth chosen is the one minimising this. However, the total discrepancy score is the sum of the individual discrepancies. A very large individual discrepancy associated with an observation suggests it is very different - in a multidimensional way, to the observations near to it.

```{r 04_discrep1, fig.height=3, fig.width=8}
## Plot the discrepancies as boxplot
plotGWPCA_discr(pcagw, type = "box")
```

```{r 04_discrep2}
## Plot the discrepancies map
plotGWPCA_discr(pcagw, type = "map")
```

```{r 04_discrep3}
## Get location data for the discrepancies
discrepancy_loc_dt <- getDiscrepancyLocData(m_sfe = meta_sfe, 
                                            gwpca = pcagw, 
                                            sample_id = "JBO019")
```

Another possibility to understand the nature of the outlier is a parallel coordinates heatmap. Here, each observation neighbouring the location that has been found to be an outlier is shown as a column with the genes in rows. Since here we are investigating local outliers, one particular observation is highlighted in red - the outlier - and the remaining ones in grey, but with the intensity of the grey fading according to their distance from the red observation. This enables you to see what characteristic the red observation has that means it as outlying from its neighbours. The plot can be created using `STExplorerDev::plotGWPCA_discrHeatmap`:

```{r 04_discrep4, message=FALSE, fig.show='hold', fig.height=15, fig.width=9}
head(discrepancy_loc_dt)
focus <- discrepancy_loc_dt$barcodes[1:2]
bw = 3*sfe@metadata[["spotDiameter"]][["JBO019"]][["spot_diameter_fullres"]]

# Plot the heatmap to visualise the genes that make this location an outlier
plotGWPCA_discrHeatmap(m_sfe = meta_sfe,
                       assay = "logcounts",
                       vars = NULL,
                       focus = focus,
                       dMetric = "euclidean", 
                       sample_id = "JBO019",
                       bw = bw, 
                       mean.diff = 1, 
                       show.vars = "top", 
                       scale = "row", 
                       gene.names = TRUE,
                       color = rev(colorRampPalette(brewer.pal(11, "RdBu"))(1000)),
                       fontsize_row = 3)
```

```{r 04_discrep5, message=FALSE}
discrepancy_gene_dt <- getDiscrepancyGeneData(m_sfe = meta_sfe,
                                              assay = "logcounts",
                                              vars = NULL,
                                              focus = focus[2],
                                              dMetric = "euclidean", 
                                              sample_id = "JBO019",
                                              bw = bw, 
                                              mean.diff = 1, 
                                              show.vars = "top",
                                              exportExpression = TRUE)
head(discrepancy_gene_dt)
```

### Functional clustering

Further utilising the loading scores, ranked lists of genes can be generated. Through this approach, we can investigate what is the functional impact of these leading genes locally. The ranked lists can be used to initially perform functional annotation per location and subsequently, these annotations are used to cluster together similarly behaving locations and identify which processes or pathways are affected in these locations. Gene Set Enrichment Analysis (GSEA) utilising the Molecular Signatures Database (MSigDB) can help us achieve this in this case. In general, though, any form of gene-related annotation can be used.

The first step in this process is to download the data from the MSigDB. Alternatively a user may skip this step if they already have a term-to-gene 2-column data frame.

```{r functional_clust1, message=FALSE}
msigdb <- getMSigDBData("Homo sapiens")

```

If the MSigDB is used, it is advisable to check the different classes and sub-classes that exist within the database we just downloaded.

```{r functional_clust2, message=FALSE}
viewCollections()

```

The next step is to generate the term-to-gene, 2-column data frame. Again here, a user may skip this step if they already have a term-to-gene 2-column data frame.

```{r functional_clust3, message=FALSE}
t2g <- getTerm2Gene(msig_data = msigdb, cat = "C2", subcat = "CP")

```

Now that we have a term-to-gene data frame, we can perform functional clustering

```{r functional_clust4, message=FALSE}
gsea_map <- gwpca_FunctionalClustering(gwpca = pcagw,
                                       pc = 1,
                                       genes_no = 1,
                                       NES = 1.5,
                                       minGSSize = 5,
                                       pvalueCutoff = 0.25,
                                       TERM2GENE = t2g,
                                       pAdjustMethod = "fdr",
                                       scoreType = "std",
                                       nPermSimple = 10000,
                                       mc.cores = 4)

```

Let's have a look at the individual arguments of the `gwpca_FunctionalClustering()` function:

1.  `gwpca`: A GWPCA object containing the results of the spatial transcriptomics analysis.\
2.  `pc`: The principal component (PC) index to be used for GSEA. This indicates the PC to be used to extract information out of GWPCA to perform GSEA.\
3.  `genes_no`: The minimum number of genes in a gene set to be considered for enrichment.\
4.  `NES`: The minimum Normalized Enrichment Score (NES) for considering a gene set as enriched.\
5.  `minGSSize`: The minimum gene set size to be considered in GSEA.\
6.  `pvalueCutoff`: The p-value cutoff for identifying significant gene sets in GSEA.\
7.  `TERM2GENE`: The term-to-gene mapping for gene sets. It has to be a 2-column data frame. The first column, named 'term' needs to include the ontology term while the second column, named 'gene', needs to include the genes present in an ontology term. Each row must have a single pair of one term and one gene. As a result, for a term which includes 10 genes, the term-to-gene data frame will have 10 rows. One row for each gene in this term.\
8.  `pAdjustMethod`: The method for multiple testing correction in GSEA (default is "fdr"). For more information on the arguments have a look at the `clusterProfiler::GSEA` documentation (use `?clusterProfiler::GSEA`).\
9.  `scoreType`: The GSEA scoring type (default is "std"). For more information on the arguments have a look at the `clusterProfiler::GSEA` documentation (use `?clusterProfiler::GSEA`).\
10. `scoreType`: The number of permutations for simple GSEA.\
11. `mc.cores`: The number of cores to use for parallel processing.\
12. `regex` A regular expression pattern for cluster identification. There can be cases where the names of the pathways or the ontology terms are large and a user may want to remove parts of these names. This argument will remove the parts of the ontology names that match the regular expression.\

Now we can plot the Functional Clustering results.

```{r functional_clust5, message=FALSE}
plotGWPCA_FuncCLust(gsea_map, count = 5, legend = "right")

```

In our example above, we used the Canonical Pathways (CP) module from the MSigDB to run GSEA in each location. After filtering the results per location to remove low-quality results, we selected the most enriched CP to label each location and perform the clustering. However, GSEA provides more information that accompanies each enriched term which assists us in making more informative decisions and increases the depth of our understanding of the result. For this reason, we also plot maps that show normalised enrichment scores, the adjusted p-values, the rank of the gene list, the gene set size, and the number of genes from the ranked list present in each canonical pathway. In the analysis workflow we repeat the functional clustering with a series of gene sets depending on the tissue studied and the biological questions present. Here we give two examples of additional functional clustering using the Hallmark Gene Sets and Liver Cell-type Signatures from the MSigDB.

## Fuzzy Geographically Weighted Clustering (FGWC)

### Background

FGWC is a supervised clustering method that, for each point in space, assigns partial membership to multiple classes. The big difference between FGWC and classic clustering is the assumption that biological clusters do not always have hard boundaries but in many cases, an ecotone exists around each cluster area that generates a gradient between two adjacent clusters. Much like the geographical ecotones we see in nature biological ecotones are either present naturally or can be introduced by the platform used to conduct the experiment.

Usually, when clustering single-cell RNA-sequencing (scRNA-seq) data researchers look to cluster together cells based on their cell type and subtype. In spatial transcriptomics, researchers usually look to identify regions of the tissue that match the existing information of the histopathology expert annotation or reveal hidden patterns in the microenvironment that are not visible through microscopy only.

Fuzzy Geographically Weighted Clustering (FGWC) was developed by Mason and Jacobson (2007) by adding neighborhood effects and population to configure the membership matrix in Fuzzy C-Means. There are seven optimisation algorithms that currently provided in this package, mainly from the Yang (2014). The optimization algorithm uses the centroid as the parameter to be optimized. Here are the algorithm that can be used:

1.  **"classic"** - The classical algorithm of FGWC based on Mason and Jacobson (2007) for centroid optimisation and Runkler and Katz (2006) for membership optimization.\

2.  **"abc"** - Optimization using artificial bee colony algorithm based on Karaboga and Basturk (2007) (see also Wijayanto and Purwarianti 2014 and Wijayanto et al. 2016 for FGWC implementation).\

3.  **"fpa"** - Optimization using flower pollination algorithm based on (Yang 2012).\

4.  **"gsa"** - Optimization using gravitational search algorithm based on Rashedi et al. (2009) and Li and Dong (2017) (see also Pamungkas and Pramana 2019 for FGWC implementation).\

5.  **"hho"** - Optimization using harris-hawk optimization with "heidari" (Heidari et al. 2019) (default). and "bairathi" (Bairathi and Gopalani 2018).\

6.  **"ifa"** - Optimization using intelligent firefly algorithm based on Yang (2009), as well as the intelligent improvement by Fateen and Bonilla-Petriciolet (2013) (see also Nasution et al. 2020 for FGWC implementation).\

7.  **"pso"** - Optimization using particle swarm optimization based on Runkler and Katz (2006) and Bansal et al. (2011) for inertia option (see also Wijayanto and Purwarianti 2014; Putra and Kurniawan 2017; Abdussamad 2020 for FGWC implementation).\

8.  **"tlbo"** - Optimization using teaching - learning based optimization based on Rao et al. (2012) and elitism improvement by Rao and Patel (2012).\

Furthermore, there are 10 distance that can be used to calculate the membership (see cdist for details). the default parameter of FGWC (in case you do not want to tune anything) is

```{r fgwc_params1, eval=FALSE}
c(kind = 'u', ncluster = 2, m = 2, distance = 'euclidean', order = 2, alpha = 0.7, a = 1, b = 1,
max.iter = 500, error = 1e-5, randomN = 1)
```

There is also a universal parameter to the optimization algorithm as well as the details. The default parameter for the optimization algorithm is

```{r fgwc_params2, eval=FALSE}
c(vi.dist = 'uniform',  npar = 10, par.no = 2, par.dist = 'euclidean', par.order = 2, pso = TRUE, 
same = 10, type = 'sim.annealing', ei.distr = 'normal', vmax = 0.7, wmax = 0.9, wmin = 0.4, 
chaos = 4, x0 = 'F', map = 0.7, ind = 1, skew = 0, sca = 1)
```

If you do not define a certain parameter, the parameter will be set to its default value (the values above).

### Reduce dimensions using NMF

```{r fgwc_nmf}
sfe_nmf <- fgwc_nmf(sfe, sample_id = "JBO019", top_hvgs = top_hvgs)

```

### Select FGWC parameters

```{r fgwc_params}
fgwc_param <- fgwc_params(algorithm = "classic", ncluster = 5)

```

### Run FGWC

```{r fgwc}
fgwc <- fgwcSTE(sfe, "JBO019", data = sfe_nmf, dMetric = "euclidean", fgwc_param = fgwc_param)

```

### Plot the highest membership clusters

```{r fgwc_plot-single}
plotFGWC_single(fgwc = fgwc, m_sfe = sfe, sample_id = "JBO019")

```

Since FGWC is a method that assigns membership percentages to each cluster, we can select and plot the cluster with the highest percentage of membership in each location and create a map of clusters. If we contrast the highest membership cluster map with the tissue histology (see below) and the scRNA-seq-based annotation we will see that the clustering result resembles, at least, the annotation.

```{r fgwc-tisue_annot, echo=FALSE, out.height="20%", fig.show='hold',fig.align='center', fig.cap="Left: sample image. Right: sample annotation."}
knitr::include_graphics("./images/tissue_annotation_gtruth.png")
```

### Plot the membership percentages

Nevertheless, this is not an accurate depiction of the clustering results. A more accurate representation is the one shown below. There, we show maps of percentages from all clusters side-by-side, and we can investigate the ecotones that might exist between clusters.

```{r fgwc_plot-multi}
plotFGWC_multi(fgwc = fgwc, m_sfe = sfe, sample_id = "JBO019")

```

**What is an ecotone?** - An ecotone in geographical sciences is a blurred gradient between a forest area and a grassland. Most of the times, a forest area doesn't stop abruptly to become a grassland but slowly the forest is thinning out to give its place to the grassland. The same idea can be found in spatial transcriptomics too. There might be genes or cells that their expression or presence in space does not form coherent areas but blends with the surrounding areas in a smooth-like transition from one gene expression to the other or from one cell type to the other. Additionally, the fact that the spatial transcriptomics platforms are yet not in single cell level, means that we have an artificial introduction of ecotones. One example is the 10X Genomics Visium platform where each spot can include 2-10+ cells depending on tissue density. This means that a spot can overlap different tissue zones leading to an abnormal clustering if we don't consider ecotones.

```{r  fgwc_ecotones, echo=FALSE, out.height="20%", fig.show='hold',fig.align='center', fig.cap="The ecotone is an area of transition between two areas of different morphology. We can find such ecotones in data from low-resolution spot-based technologies like Visium."}
knitr::include_graphics("./images/ecotones.png") 
```

In the multiple membership plot earlier we can see an example of ecotones. Clusters 3 and 5 present the ecotone as a blurred gradient of intermediate percentages between the two clusters. The ecotone though is not always the same size. For example, in clusters like cluster 4, the ecotone is much thinner.

### Plot a heatmap of established biomarkers

For this plot to be generated, it required from the user to provide a data frame (table) with four (4) columns that includes the below information and the specific column names:

1.  **"gene.name"**: gene names (e.g., APOE)\
2.  **"ensg.ID"**: ENSGene IDs (e.g., ENSG00000130203)\
3.  **"Type"**: Cell type (e.g., Hepatocytes)\
4.  **"Subtype"**: Cell subtype (e.g., Stellate cells)\

If no subtype exists, add an empty character string using: `""`

**NOTE!:** if the column names do not match, an error will be returned with a prompt to fix them.

```{r fgwc_plot-heatmap}
## Load the liver markers example dataset
markers <- data(markers)

## Plot the heatmap
heatmap <- plotFGWC_heatmap(fgwc = fgwc, m_sfe = sfe, sample_id = "JBO019", markers = markers, cluster_no = 3)

```

In cases where the unit of measurement is an area encompassing more than one cell (i.e., 10X Visium spots), fuzzy clustering looks more appropriate. Indeed, if we look at cell-type markers in cluster 4 we see that each location has a mixture of different cell-type markers indicating that a mixture of cells exists in each spot and thus absolute clustering often fails to capture the high levels of heterogeneity present. This idea of an ecotone and of a gradient rather than absolute cluster boundaries more closely captures the different types of cells that might co-exist in a 10X Visium spot and also leads to a more dynamic clustering that can reveal the existing spatial heterogeneity.

### Visualise subclusters

This approach generates a blurred clustering of the data points and provides us with a way to see inside the spots. Utilising established cell-type markers, specific to the liver tissue, we can examine the intra-cluster cell-type heterogeneity that the ecotone suggested, by subdividing each cluster into subclusters according to their cell-type mixture.

```{r  fgwc_plot-subClust}
plotFGWC_subClust(heatmap = heatmap, k = 5, clust = 3, m_sfe = sfe, sample_id = "JBO019")

```

### Plot a heatmap for selected subclusters

To understand the composition of each subcluster we can plot a heatmap using the same established markers.

```{r  fgwc_plot-subClust-heatmap}
## Only plot the sub-cluster heatmap
plotFGWC_subHeatmap(heatmap = heatmap, k = 5, markers = markers, m_sfe = sfe, sample_id = "JBO019", cluster_no = 4)

## Save the heatmap as an object
subHeatmap <- plotFGWC_subHeatmap(heatmap = heatmap, k = 5, markers = markers, m_sfe = sfe, sample_id = "JBO019", cluster_no = 4)
```

### References

Abdussamad S (2020). "Evaluation of Implementation Context Based Clustering In Fuzzy Geographically Weighted Clustering-Particle Swarm Optimization Algorithm." Jurnal EECCIS, 14(1), 10--15. ISSN 2460-8122, <https://jurnaleeccis.ub.ac.id/index.php/eeccis/article/view/609>.

Bairathi D, Gopalani D (2018). "A Novel Swarm Intelligence Based Optimization Method: Harris' Hawk Optimization." In Advances in Intelligent Systems and Computing, 832--842. Springer International Publishing. doi: 10.1007/978-3-030-16660-1_81, <https://doi.org/10.1007/978-3-030-16660-1_81>.

Bansal JC, Singh PK, Saraswat M, Verma A, Jadon SS, Abraham A (2011). "Inertia Weight strategies in Particle Swarm Optimization." In 2011 Third World Congress on Nature and Biologically Inspired Computing. doi: 10.1109/nabic.2011.6089659, <https://doi.org/10.1109/nabic.2011.6089659>.

Fateen SK, Bonilla-Petriciolet A (2013). "Intelligent Firefly Algorithm for Global Optimization." Cuckoo Search and Firefly Algorithm: Theory and Applications, 516, 315--330.

Heidari AA, Mirjalili S, Faris H, Aljarah I, Mafarja M, Chen H (2019). "Harris hawks optimization: Algorithm and applications." Future Generation Computer Systems, 97, 849--872. doi: 10.1016/j.future.2019.02.028, <https://doi.org/10.1016/j.future.2019.02.028>.

Karaboga D, Basturk B (2007). "A powerful and efficient algorithm for numerical function optimization: artificial bee colony (ABC) algorithm." Journal of Global Optimization, 39(3), 459--471. doi: 10.1007/s10898-007-9149-x, <https://doi.org/10.1007/s10898-007-9149-x>.

Li J, Dong N (2017). "Gravitational Search Algorithm with a New Technique." In 2017 13th International Conference on Computational Intelligence and Security (CIS), 516--519. doi: 10.1109/CIS.2017.00120, <https://doi.org/10.1109/CIS.2017.00120>.

Mason GA, Jacobson RD (2007). "Fuzzy Geographically Weighted Clustering." In Proceedings of the 9th International Conference on Geocomputation, 1--7.

Nasution BI, Kurniawan R, Siagian TH, Fudholi A (2020). "Revisiting social vulnerability analysis in Indonesia: An optimized spatial fuzzy clustering approach." International Journal of Disaster Risk Reduction, 51, 101801. doi: 10.1016/j.ijdrr.2020.101801, <https://doi.org/10.1016/j.ijdrr.2020.101801>.

Pamungkas IH, Pramana S (2019). "Improvement Method of Fuzzy Geographically Weighted Clustering using Gravitational Search Algorithm." Journal of Computer Science and Information, 11(1).

Putra FH, Kurniawan R (2017). "Clustering for Disaster Areas Endemic Dengue Hemorrhagic Fever Based on Factors had Caused in East Java Using Fuzzy Geographically Weighted Clustering - Particle Swarm Optimization." Jurnal Aplikasi Statistika & Komputasi Statistik, 8(01), 27. ISSN 2615-1367.

Rao RV, Patel V (2012). "An elitist teaching-learning-based optimization algorithm for solving complex constrained optimization problems." International Journal of Industrial Engineering Computations, 3(4), 535--560. ISSN 19232926, doi: 10.5267/j.ijiec.2012.03.007, <https://doi.org/10.5267/j.ijiec.2012.03.007>.

Rao RV, Savsani VJ, Balic J (2012). "Teaching- learning-based optimization algorithm for unconstrained and constrained real-parameter optimization problems." Engineering Optimization, 44(12), 1447--1462. doi: 10.1080/0305215x.2011.652103, <https://doi.org/10.1080/0305215x.2011.652103>.

Rashedi E, Nezamabadi-pour H, Saryazdi S (2009). "GSA: A Gravitational Search Algorithm." Information Sciences, 179(13).

Runkler TA, Katz C (2006). "Fuzzy Clustering by Particle Swarm Optimization." In 2006 IEEE International Conference on Fuzzy Systems. doi: 10.1109/fuzzy.2006.1681773, <https://doi.org/10.1109/fuzzy.2006.1681773>.

Wijayanto AW, Purwarianti A (2014). "Improvement design of fuzzy geo-demographic clustering using Artificial Bee Colony optimization." In 2014 International Conference on Cyber and IT Service Management (CITSM), 69--74. ISBN 978-1-4799-7975-2.

Wijayanto AW, Purwarianti A (2014). "Improvement of fuzzy geographically weighted clustering using particle swarm optimization." In 2014 International Conference on Information Technology Systems and Innovation (ICITSI), 7--12. ISBN 978-1-4799-6527-4.

Wijayanto AW, Purwarianti A, Son LH (2016). "Fuzzy geographically weighted clustering using artificial bee colony: An efficient geo-demographic analysis algorithm and applications to the analysis of crime behavior in population." Applied Intelligence, 44(2), 377--398. ISSN 0924-669X.

Yang X (2014). Nature-Inspired Optimization Algorithms, Elsevier insights. Elsevier Science. ISBN 9780124167452.

Yang X (2012). "Flower Pollination Algorithm for Global Optimization." In Unconventional Computation and Natural Computation, 240--249. Springer Berlin Heidelberg. doi: 10.1007/978-3-642-32894-7_27, <https://doi.org/10.1007/978-3-642-32894-7_27>.

Yang X (2009). "Firefly Algorithms for Multimodal Optimization." In Stochastic Algorithms: Foundations and Applications, 169--178. Springer Berlin Heidelberg. doi: 10.1007/978-3-642-04944-6_14, <https://doi.org/10.1007/978-3-642-04944-6_14>.

## Geographically Weighted Regression

### GWR

## Spatial Autocorrelation

When we are performing spatial analysis in research, we're looking to achieve one (or more) of mainly three things:

1.  **Analyse the distribution of a phenomena**
2.  **Analyse the relationship between phenomena, to identify factors/predictors**
3.  **Create new spatial data that can then be used in the above**

When you attach values to the polygons of spots or cells in a spatial omics dataset, and visualise them, different patterns appear, and the different shapes and sizes of the polygons effect what these patterns look like and how prominent they are to the human eye.

There can appear to be clusters, or, in some cases, the distribution can be random.

When values are clustered, this issue is known as spatial autocorrelation and results from what is known as **spatial dependence**.

The concept of spatial autocorrelation has existed for many years. It is captured in what has become known as Tobler’s “first law of geography” which states that

> “Everything is related to everything else, but near things are more related than distant things.” *Waldo R. Tobler (1970)*

This reflects what we intuitively know about the world we live in —phenomena are clustered and not randomly distributed— and implies some form of distance decay in observation values and that measurements of a phenomenon will be correlated over space. This lack of observation independence has to be accounted for with statistical models.

Several established tests for spatial autocorrelation exist for measurements in a spatially resolved dataset which all have a similar underlying operation. Essentially, they generate measures of similarity by comparing each observation value with those in its neighbourhood. The neighbourhood may be defined by a distance or some form of adjacency (1st order to nth order), and the spatial autocorrelation test may report local measures of spatial autocorrelation or a global (whole map) one which is essentially summarized from local measures. For example, in spatial transcriptomics, a global measure will indicate whether a gene exhibits spatial autocorrelation or not, while a local measure will reveal the exact pattern of said autocorrelation in space. The advantages of the local measures are that they indicate the locales of spatial autocorrelation within the data, critically the local significance of the spatial autocorrelation test. In order to understand the local measures of spatial autocorrelation, we first need to consider their global counterparts [@Zormpas2023Cell].

### Global and local measures

> "**Global spatial autocorrelation measures**: statistics assessing the degree of spatial autocorrelation for a variable in the whole dataset."

> "**Local spatial autocorrelation measures**: statistics assessing the degree of spatial autocorrelation for a variable in every location compared to its neighbours."

Moran’s correlation coefficient, known as Moran’s I, is a univariate measure of spatial autocorrelation. It is the most well-recognized and widely used spatial autocorrelation statistic. I can take values between −1 and +1, where values near +1 indicate strong positive spatial autocorrelation, values near −1 indicate strong negative spatial autocorrelation (i.e., perfect dispersion), and values near zero suggest a randomly dispersed variable [@Zormpas2023Cell].

### Moran's I

```{r moran-global}
sfe <- moranGlobalIPerm(sfe, sample_id = "JBO019", genes = TRUE, mc.cores = 1)

head(rowData(sfe))

```

Once we calculate the Global Moran's I we can use the pseudo-pvalue generated by permutation and select only the genes that are most likely to be affected by spatial autocorrelation.

The below chunk of code allows the user to create a named vector of the genes that pass the selection provided filters:

1.  `pVal` : is the p-value threshold that will allow only genes with a pseudo p-value less than the filter to pass.

2.  `stat_thresh` : is the global statistic threshold that will allow only genes with a global statistic greater than the threshold to pass.

At the end, both filters need to be `TRUE` for a gene to be present in the final named vector. The same logic will be applied for Geary's C and Getis & Ord's G in the next chapters.

```{r moran-select}
gs <- getSAGlobalGenes()

```

Now that we selected the genes that exhibit putatively interesting patterns in space, we can calculate the local measure of SA (*here local Moran's I*) and visualise the results on a map of the tissue section.

```{r moran-local}
sfe <- moranLocalIPerm(m_sfe = sfe,
                       sample_id = "JBO019",
                       genes = gs,
                       mc.cores = 8)
```

When mapping the local statistics you can present it in two ways:

1.  All measurements and significant ones highlighted with a grey-white line.

2.  Only the significant measurements with the rest of the tissue greyed-out.

This is selected by providing to the `locations` argument either `"all"` (for option 1) or `"significant"` (for option 2).

```{r moran-plot}
for (g in gs) {
  p <- plotSA_local(m_sfe = sfe, 
             sample_id = "JBO019",
             feature = g,
             statistic = "moran",
             test = "permutation",
             pVal = 0.05,
             type = "hex",
             title = "name",
             locations = "significant")
  
  print(p)
}

for (g in gs) {
  p <- plotSA_localClust(m_sfe = sfe, 
             sample_id = "JBO019",
             feature = g,
             statistic = "moran",
             test = "permutation",
             pVal = 0.05,
             type = "hex",
             title = "name",
             clust_col = NULL)
  
  print(p)
}
```

### Geary's C

Alternative measures to Moran’s I include Geary’s C and Getis and Ord’s G statistic. Geary’s C takes only positive values, where C = 1 means that there is a lack of spatial autocorrelation. Values of C approaching 0 show increasing positive spatial autocorrelation, and C ≫ 1 is indicative of negative spatial autocorrelation which suggests large differences between a location and its neighbours. Thus, Geary’s C is inversely related to Moran’s I, though as the measures are calculated differently, they are not directly related. Due to these differences in calculation, Moran’s I is more a global measure and more sensitive to extreme values while Geary’s C is sensitive to differences between values in neighbouring areas [@Zormpas2023Cell].

```{r geary-global}
sfe <- gearyGlobalCPerm(sfe, sample_id = "JBO019", genes = TRUE, mc.cores = 8)

head(rowData(sfe))
```

```{r moran-select}
gs <- getSAGlobalGenes()

```

```{r moran-local}
sfe <- moranLocalIPerm(m_sfe = sfe,
                       sample_id = "JBO019",
                       genes = gs,
                       mc.cores = 8)
```

```{r moran-plot}
for (g in gs) {
  p <- plotSA_local(m_sfe = sfe, 
             sample_id = "JBO019",
             feature = g,
             statistic = "moran",
             test = "permutation",
             pVal = 0.05,
             type = "hex",
             title = "name",
             locations = "significant")
  
  print(p)
}


for (g in gs) {
  p <- plotSA_localClust(m_sfe = sfe, 
             sample_id = "JBO019",
             feature = g,
             statistic = "moran",
             test = "permutation",
             pVal = 0.05,
             type = "hex",
             title = "name",
             clust_col = NULL)
  
  print(p)
}
```

### Getis & Ord's G

The G statistic is ideal for checking whether similar values co-locate as it only provides a measure of positive spatial autocorrelation. It is more useful as a local model of spatial autocorrelation, as discussed in the next section [@Zormpas2023Cell].

```{r getis}

```

## Putting it all together

The below code puts all these steps in order by selecting one of the options at each step.

```{r all_in_one, message=FALSE, warning=FALSE, eval=FALSE}
## Import data
# sampleDir <- "./data/spaceranger_outs/Human_Liver_Steatotic/JBO019_Results"
sampleNames <- "JBO019"
sfe <- read10xVisiumSFE(samples = sampleDir, 
                        sample_id = sampleNames, 
                        type = "sparse", 
                        data = "filtered", 
                        images = "lowres", 
                        style = "W", 
                        zero.policy = TRUE)
# ----------------------------------------------- #
# ground_truth <- read_table("./data/to_load/spotzonationGroup.txt")
## Add QC metrics
is_mito <- grepl("(^MT-)|(^mt-)", rowData(sfe)$symbol)
sfe <- addPerLocQC(sfe, gTruth = ground_truth, assay = "counts", 2, subsets = list(mito = is_mito))
sfe <- addGeometries(sfe, samples = sampleDir, sample_id = sampleNames, res = "fullres")
sfe <- addPerGeneQC(sfe, assay = "counts", version = NULL, mirror = NULL)
# ----------------------------------------------- #
## SPOT SELECTION
## Select library size threshold
qc_lib_size <- colData(sfe)$sum < 1000
## Add threshold in colData
colData(sfe)$qc_lib_size <- qc_lib_size
## Select expressed genes threshold
qc_detected <- colData(sfe)$detected < 550
## Add threshold in colData
colData(sfe)$qc_detected <- qc_detected
## Select mitochondrial percentage threshold
qc_mito <- colData(sfe)$subsets_mito_percent > 22
## Add threshold in colData
colData(sfe)$qc_mito <- qc_mito
## Combine together the set of discarded spots
discard <- qc_lib_size | qc_detected | qc_mito
## Store the set in the object
colData(sfe)$discard <- discard
## Remove combined set of low-quality spots
sfe <- sfe[, !colData(sfe)$discard]
# ----------------------------------------------- #
## FEATURE SELECTION
## Calculate library size factors
sfe <- computeLibraryFactors(sfe)
## Calculate logcounts using library size factors
sfe <- logNormCounts(sfe)
## Calculate log-counts sample mean
rowData(sfe)[["JBO019.s_logMean"]] <- rowSums(assay(sfe, "logcounts")) / rowData(sfe)[["JBO019.nLocations"]]
## Set and apply filters
is_zero <- rowData(sfe)$total == 0
is_logLow <- rowData(sfe)[["JBO019.s_logMean"]] <= 1
discard_gs <- is_zero | is_mito | is_logLow
rowData(sfe)$discard <- discard_gs
## Remove mitochondrial and other genes
sfe <- sfe[!rowData(sfe)$discard, ]
## Fit mean-variance relationship
dec <- modelGeneVar(sfe,
                    assay.type = "logcounts")
## Select top HVGs
top_hvgs <- getTopHVGs(dec, 
                       var.field = "bio", 
                       prop = 0.5,
                       var.threshold = 0,
                       fdr.threshold = 0.05)
# ----------------------------------------------- #
## ADD GEOGRAPHY
## Add a neighbour graph using a weighted distance matrix
sfe <- addSpatialNeighGraphs(sfe, "JBO019", type = "knearneigh", style = "W", distMod = "raw", k = 6)
## Calculate a simple distance matrix
sfe <- addDistMat(sfe, p = 2)
```

# References {.unnumbered}

Here is the output of `sessionInfo()` on the system on which this document was compiled

```{r echo=FALSE}
sessionInfo()
```
